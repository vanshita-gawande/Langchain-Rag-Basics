from dotenv import load_dotenv
import os

from langchain_community.document_loaders import WebBaseLoader
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

#  Load Hugging Face token
load_dotenv()

hf_token = os.getenv("HUGGINGFACEHUB_ACCESS_TOKEN")

if not hf_token:
    raise ValueError("‚ùå Token not found in .env file")

#  1Ô∏è‚É£ LOAD TEXT FILE USING TextLoader
loader = WebBaseLoader("https://en.wikipedia.org/wiki/Apple_Watch")
documents = loader.load()

#  Extract only the text from the document
text_data = documents[0].page_content

# ‚úÖ üî• IMPORTANT: LIMIT TEXT TO AVOID TOKEN ERROR
text_data = text_data[:4000]   # ‚úÖ SAFE LIMIT

print("\n FILE LOADED SUCCESSFULLY!\n")
print(text_data[:10]) #first 10 characters

# 2Ô∏è‚É£ CREATE HUGGING FACE MODEL
base_llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    huggingfacehub_api_token=hf_token,
    temperature=0.3,
    max_new_tokens=200,
)

llm = ChatHuggingFace(llm=base_llm)

# 3Ô∏è‚É£ CREATE SIMPLE PROMPT
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("human", "what is the name of course answer from this :\n{text}") #then pass the question while invoking in text
])

# 4Ô∏è‚É£ OUTPUT PARSER
parser = StrOutputParser()

# 5Ô∏è‚É£ SIMPLE CHAIN (Prompt ‚Üí LLM ‚Üí Parser)
chain = prompt | llm | parser

# 6Ô∏è‚É£ RUN THE CHAIN WITH FILE CONTENT
result = chain.invoke({"text": text_data})

print("\n AI SUMMARY OUTPUT:\n")
print(result)

